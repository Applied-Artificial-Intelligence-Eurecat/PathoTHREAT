# -*- coding: utf-8 -*-

from flask import Flask, request
import tensorflow as tf
from langchain.chains import RetrievalQA
# -*- coding: utf-8 -*-
# https://colab.research.google.com/drive/1PHH29WwFl0hrA5Tv5FGfjhX7KKjjCifd
from langchain.document_loaders import TextLoader
from langchain.embeddings import HuggingFaceEmbeddings
from langchain.llms import HuggingFacePipeline
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.vectorstores import Chroma
from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline
import time

task = "text-generation"
access_token = "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"

loader = TextLoader("article.txt")
doc = loader.load()

text_splitter = RecursiveCharacterTextSplitter(chunk_size=600, chunk_overlap=100)
all_splits = text_splitter.split_documents(doc)

model_name = "sentence-transformers/all-MiniLM-L6-v2"
model_kwargs = {'device': 'cpu'}
encode_kwargs = {'normalize_embeddings': False}
hf = HuggingFaceEmbeddings(
    model_name=model_name,
    model_kwargs=model_kwargs,
    encode_kwargs=encode_kwargs
)

vectorstore = Chroma.from_documents(documents=all_splits, embedding=hf)

model_id = "meta-llama/Llama-2-7b-hf"
tokenizer = AutoTokenizer.from_pretrained(model_id, use_auth_token=access_token)
model = AutoModelForCausalLM.from_pretrained(model_id, use_auth_token=access_token)
if len(tf.config.list_physical_devices('GPU')) == 0:
    pipe = pipeline(
        task, model=model, tokenizer=tokenizer, max_new_tokens=100, use_auth_token=access_token, top_p=0.8, top_k=20, length_penalty=-0.05
    )
else:
    pipe = pipeline(
        task, model=model, tokenizer=tokenizer, max_new_tokens=100, use_auth_token=access_token, top_p=0.8, top_k=20, length_penalty=-0.05, device=0
    )
hf = HuggingFacePipeline(pipeline=pipe)

qa_chain = RetrievalQA.from_chain_type(hf, retriever=vectorstore.as_retriever())

app = Flask(__name__)


def prompt(query):
    return f"""
    System: You are a helpful, respectful and honest assistant. Always answer as helpfully as possible, while being safe.  Your answers should not include any harmful, unethical, racist, sexist, toxic, dangerous, or illegal content. Please ensure that your responses are socially unbiased and positive in nature. If a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct. If you don't know the answer to a question, please don't share false information.
    User: {query}
    System: """


model_path = "meta-llama/Llama-2-7b-hf"
# model_path = "gpt2"


@app.route('/test', methods=['POST'])
def test():
    return "El Flask arribe almenos"

@app.route('/', methods=['POST'])
def process_query():
    data = request.get_json()
    input_query = data.get('query', '')
    start_time = time.perf_counter()
    query_answer = pipe(prompt(input_query), return_full_text=False)
    print(f"Time passed: {time.perf_counter()-start_time:.6f} seconds")
    st = query_answer[0]["generated_text"]
    return {'answer': st}


@app.route('/langchain', methods=['POST'])
def process_langchain():
    data = request.get_json()
    input_query = data.get('query', '')
    start_time = time.perf_counter()
    ans = {'answer': qa_chain({"query": input_query})}
    print(f"Time passed: {time.perf_counter()-start_time:.6f} seconds")
    return ans


if __name__ in "__main__":
    app.run(host='0.0.0.0')
